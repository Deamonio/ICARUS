import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from ultralytics import YOLO
from PIL import Image
import matplotlib.pyplot as plt
import csv
import numpy as np
from datetime import datetime
import glob
import random
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2

# ==========================================
# 1. ë°ì´í„°ì…‹ ì •ì˜ (ì¢Œí‘œ ë³€í™˜ ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ)
# ==========================================
class CupDataset(Dataset):
    def __init__(self, img_dir, label_dir, img_size=640, augment=False):
        self.img_dir = img_dir
        self.label_dir = label_dir
        self.img_size = img_size
        self.augment = augment
        
        # ì¦ê°• ì„¤ì • (ì´ë¯¸ì§€ + ë°•ìŠ¤ + í¬ì¸íŠ¸ë¥¼ ë™ì‹œì— ë³€í˜•)
        if self.augment:
            self.transform = A.Compose([
                A.Resize(img_size, img_size),
                A.HorizontalFlip(p=0.5),
                A.RandomBrightnessContrast(p=0.2),
                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),
                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
                ToTensorV2()
            ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),
               bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
        else:
            self.transform = A.Compose([
                A.Resize(img_size, img_size),
                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
                ToTensorV2()
            ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),
               bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))

        # ìœ íš¨í•œ ìƒ˜í”Œë§Œ í•„í„°ë§ (ì´ë¯¸ì§€ ì¡´ì¬ + ë¼ë²¨ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ í˜•ì‹)
        all_images = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))]
        self.valid_samples = []
        invalid_count = 0
        
        for img_name in all_images:
            label_path = os.path.join(label_dir, img_name.rsplit('.', 1)[0] + '.txt')
            if not os.path.exists(label_path):
                invalid_count += 1
                continue
            
            # ë¼ë²¨ íŒŒì¼ ê²€ì¦
            try:
                with open(label_path, 'r') as f:
                    line = f.readline().split()
                    if len(line) >= 7:  # class + 4 bbox + 2 keypoints
                        self.valid_samples.append(img_name)
                    else:
                        invalid_count += 1
            except:
                invalid_count += 1
        
        if invalid_count > 0:
            print(f"Filtered out {invalid_count} invalid samples. Valid samples: {len(self.valid_samples)}")

    def __len__(self):
        return len(self.valid_samples)

    def __getitem__(self, idx):
        img_name = self.valid_samples[idx]
        img_path = os.path.join(self.img_dir, img_name)
        label_path = os.path.join(self.label_dir, img_name.rsplit('.', 1)[0] + '.txt')
        
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h_orig, w_orig = image.shape[:2]

        with open(label_path, 'r') as f:
            line = f.readline().split()
            
            # ë¼ë²¨ íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš° ì²´í¬
            if len(line) < 7:
                raise ValueError(f"Label file {label_path} has insufficient data. Expected at least 7 values, got {len(line)}")
            
            # YOLO í˜•ì‹: [cx, cy, w, h]
            cx, cy, w, h = [float(x) for x in line[1:5]]
            
            # YOLO -> xyxy ë³€í™˜í•˜ì—¬ ë°•ìŠ¤ê°€ ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šë„ë¡ í´ë¨í”„
            x_min = np.clip(cx - w/2, 0.0, 1.0)
            y_min = np.clip(cy - h/2, 0.0, 1.0)
            x_max = np.clip(cx + w/2, 0.0, 1.0)
            y_max = np.clip(cy + h/2, 0.0, 1.0)
            
            # ë‹¤ì‹œ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ ì™„ì „ ì œê±°ë¥¼ ìœ„í•´ í•œë²ˆ ë” í´ë¨í”„)
            cx_clipped = np.clip((x_min + x_max) / 2, 0.0, 1.0)
            cy_clipped = np.clip((y_min + y_max) / 2, 0.0, 1.0)
            w_clipped = np.clip(x_max - x_min, 0.0, 1.0)
            h_clipped = np.clip(y_max - y_min, 0.0, 1.0)
            bbox = [cx_clipped, cy_clipped, w_clipped, h_clipped]
            
            # Point í˜•ì‹: [px, py] -> í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜í•˜ì—¬ ì¦ê°•ê¸°ì— ì „ë‹¬
            keypoint = [[np.clip(float(line[5]), 0.0, 1.0) * w_orig, 
                        np.clip(float(line[6]), 0.0, 1.0) * h_orig]]

        # ì¦ê°• ì ìš© (ì´ë¯¸ì§€, ë°•ìŠ¤, í¬ì¸íŠ¸ê°€ í•œ ëª¸ì²˜ëŸ¼ ì›€ì§ì„)
        transformed = self.transform(image=image, bboxes=[bbox], class_labels=[0], keypoints=keypoint)
        
        image = transformed['image']
        # ë³€í˜• í›„ ë‹¤ì‹œ 0~1 ì‚¬ì´ë¡œ ì •ê·œí™”ëœ ì¢Œí‘œ ì¶”ì¶œ
        box_target = torch.tensor(transformed['bboxes'][0], dtype=torch.float32)
        point_target = torch.tensor([transformed['keypoints'][0][0] / self.img_size, 
                                    transformed['keypoints'][0][1] / self.img_size], dtype=torch.float32)

        return image, box_target, point_target

# ==========================================
# 2. ëª¨ë¸ ì •ì˜ (Full Training ëª¨ë“œ)
# ==========================================
class YOLOFullTaskModel(nn.Module):
    def __init__(self):
        super(YOLOFullTaskModel, self).__init__()
        base_model = YOLO('yolov8n-pose.pt').model
        self.feature_extractor = base_model.model[:10] # Backbone
        
        feature_dim = 256
        self.box_head = self._make_head(feature_dim, 4)
        self.point_head = self._make_head(feature_dim, 2)

    def _make_head(self, in_dim, out_dim):
        return nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(in_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, out_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        return self.box_head(features), self.point_head(features)

# ==========================================
# 3. TTA ë° í‰ê°€ í•¨ìˆ˜
# ==========================================
def calculate_iou(box1, box2):
    """IoU ê³„ì‚°"""
    box1_x1 = box1[:, 0] - box1[:, 2] / 2
    box1_y1 = box1[:, 1] - box1[:, 3] / 2
    box1_x2 = box1[:, 0] + box1[:, 2] / 2
    box1_y2 = box1[:, 1] + box1[:, 3] / 2
    
    box2_x1 = box2[:, 0] - box2[:, 2] / 2
    box2_y1 = box2[:, 1] - box2[:, 3] / 2
    box2_x2 = box2[:, 0] + box2[:, 2] / 2
    box2_y2 = box2[:, 1] + box2[:, 3] / 2
    
    inter_x1 = torch.max(box1_x1, box2_x1)
    inter_y1 = torch.max(box1_y1, box2_y1)
    inter_x2 = torch.min(box1_x2, box2_x2)
    inter_y2 = torch.min(box1_y2, box2_y2)
    
    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)
    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)
    union_area = box1_area + box2_area - inter_area
    
    iou = inter_area / (union_area + 1e-6)
    return iou

def calculate_metrics(model, dataloader, device, img_size=640, use_tta=False):
    """í‰ê°€ ì§€í‘œ ê³„ì‚° (TTA ì˜µì…˜ ì¶”ê°€)"""
    model.eval()
    total_loss = 0
    total_box_loss = 0
    pixel_errors, x_errors, y_errors = [], [], []
    ious = []
    
    criterion = nn.MSELoss()
    
    with torch.no_grad():
        for images, boxes, points in dataloader:
            images, boxes, points = images.to(device), boxes.to(device), points.to(device)
            p_boxes, p_points = model(images)
            
            # Box Loss ê³„ì‚°
            box_loss = criterion(p_boxes, boxes)
            point_loss = criterion(p_points, points)
            total_box_loss += box_loss.item()
            
            # [ìˆ˜ì •] í†µí•© Loss (Boxì™€ Point ëª¨ë‘ 5.0ë°° ê°€ì¤‘ì¹˜)
            loss = (5.0 * box_loss) + (5.0 * point_loss)
            total_loss += loss.item()
            
            # IoU ê³„ì‚°
            batch_iou = calculate_iou(p_boxes, boxes)
            ious.extend(batch_iou.cpu().numpy())
            
            # í”½ì…€ ì˜¤ì°¨ ë¶„ì„
            p_points_px = p_points * img_size
            t_points_px = points * img_size
            
            diff = torch.abs(p_points_px - t_points_px)
            x_errors.extend(diff[:, 0].cpu().numpy())
            y_errors.extend(diff[:, 1].cpu().numpy())
            pixel_errors.extend(torch.sqrt((diff**2).sum(dim=1)).cpu().numpy())
    
    pixel_errors = np.array(pixel_errors)
    ious = np.array(ious)
    
    model.train()
    return {
        'mse_loss': total_loss / len(dataloader),
        'box_loss': total_box_loss / len(dataloader),
        'mean_iou': ious.mean(),
        'box_hit_rate': (ious > 0.5).mean() * 100,
        'mpe': pixel_errors.mean(),
        'x_mae': np.mean(x_errors),
        'y_mae': np.mean(y_errors),
        'pck_5': (pixel_errors <= 5).mean() * 100,
        'pck_10': (pixel_errors <= 10).mean() * 100,
        'max_error': pixel_errors.max()
    }

def plot_training_metrics(history, save_path='training_metrics.png', best_epoch=None):
    fig, axes = plt.subplots(3, 3, figsize=(20, 14))
    fig.suptitle('Transfer Learning Stage 2 - Precision Training (Box=5.0x, Point=5.0x)', fontsize=16, fontweight='bold')
    
    epochs = [h['epoch'] for h in history]
    
    # ì²« ë²ˆì§¸ ì¤„: Loss ê´€ë ¨
    axes[0, 0].plot(epochs, [h['mse_loss'] for h in history], 'b-')
    axes[0, 0].set_title('Total MSE Loss (Validation)')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    if best_epoch is not None:
        axes[0, 0].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best: {best_epoch}')
        axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    axes[0, 1].plot(epochs, [h['box_loss'] for h in history], 'purple')
    axes[0, 1].set_title('Box Loss (MSE)')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].grid(True, alpha=0.3)
    
    axes[0, 2].plot(epochs, [h['mpe'] for h in history], 'g-')
    axes[0, 2].set_title('Mean Pixel Error (MPE)')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Pixels')
    axes[0, 2].grid(True, alpha=0.3)
    
    # ë‘ ë²ˆì§¸ ì¤„: Box ê´€ë ¨
    axes[1, 0].plot(epochs, [h['mean_iou'] for h in history], 'brown')
    axes[1, 0].set_title('Mean IoU (mIoU)')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('IoU')
    axes[1, 0].set_ylim([0, 1])
    axes[1, 0].axhline(y=0.7, color='r', linestyle='--', alpha=0.5, label='Target: 0.7')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    axes[1, 1].plot(epochs, [h['box_hit_rate'] for h in history], 'darkred')
    axes[1, 1].set_title('Box Hit Rate (IoU > 0.5)')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Hit Rate (%)')
    axes[1, 1].set_ylim([0, 100])
    axes[1, 1].grid(True, alpha=0.3)
    
    axes[1, 2].plot(epochs, [h['x_mae'] for h in history], 'r-', label='X MAE')
    axes[1, 2].plot(epochs, [h['y_mae'] for h in history], 'orange', label='Y MAE')
    axes[1, 2].set_title('X/Y MAE')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Pixels')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    
    # ì„¸ ë²ˆì§¸ ì¤„: Point ê´€ë ¨
    axes[2, 0].plot(epochs, [h['pck_5'] for h in history], 'm-')
    axes[2, 0].set_title('PCK @ 5px (%)')
    axes[2, 0].set_xlabel('Epoch')
    axes[2, 0].set_ylabel('Accuracy (%)')
    axes[2, 0].set_ylim([0, 100])
    axes[2, 0].grid(True, alpha=0.3)
    
    axes[2, 1].plot(epochs, [h['pck_10'] for h in history], 'c-')
    axes[2, 1].set_title('PCK @ 10px (%)')
    axes[2, 1].set_xlabel('Epoch')
    axes[2, 1].set_ylabel('Accuracy (%)')
    axes[2, 1].set_ylim([0, 100])
    axes[2, 1].grid(True, alpha=0.3)
    
    axes[2, 2].plot(epochs, [h['max_error'] for h in history], 'k-')
    axes[2, 2].set_title('Max Pixel Error')
    axes[2, 2].set_xlabel('Epoch')
    axes[2, 2].set_ylabel('Pixels')
    axes[2, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=100)
    plt.close()

# ==========================================
# 4. ë©”ì¸ í•™ìŠµ ë£¨í”„ (ì „ì´ í•™ìŠµ 2ë‹¨ê³„ ë²„ì „)
# ==========================================
def main():
    # ========== GPU ì •ë³´ ì¶œë ¥ ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    use_cuda = torch.cuda.is_available()
    print("=" * 100)
    print(f"ğŸš€ PyTorch v{torch.__version__} / CUDA: {torch.version.cuda if use_cuda else 'N/A'}")
    if use_cuda:
        print(f"ğŸ¯ GPU: {torch.cuda.get_device_name(0)} (Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB)")
    print("=" * 100)
    
    print("\nğŸš€ ì „ì´ í•™ìŠµ 2ë‹¨ê³„ ì‹œì‘ - ì •ë°€ íŠœë‹ ëª¨ë“œ (Warm Restart)")

    # ========== ë°ì´í„°ì…‹ ë¡œë“œ ==========
    print("\nğŸ“¦ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    train_dataset = CupDataset('./dataset/images/train', './dataset/labels/train', augment=True)
    val_dataset = CupDataset('./dataset/images/val', './dataset/labels/val', augment=False)
    print(f"  Train ìƒ˜í”Œ: {len(train_dataset)}")
    print(f"  Val ìƒ˜í”Œ: {len(val_dataset)}")
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)

    # [ìˆ˜ì •] ëª¨ë¸ ì´ˆê¸°í™” í›„ ê¸°ì¡´ ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    print("\nğŸ§  ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = YOLOFullTaskModel().to(device)
    if os.path.exists('cup_model_best.pth'):
        model.load_state_dict(torch.load('cup_model_best.pth'))
        print("ğŸ”„ 1ë‹¨ê³„ ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ 2ë‹¨ê³„ ì •ë°€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("âš ï¸  cup_model_best.pthë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ˆê¸° ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    # [ìˆ˜ì •] ëŸ¬ë‹ë ˆì´íŠ¸ë¥¼ ë‹¤ì‹œ ë†’ì—¬ì„œ ì •ì²´ê¸° íƒˆì¶œ (Warm Restart)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)  # 5e-5 (0.0001ë³´ë‹¤ ì•½ê°„ ë‚®ìŒ)

    # [ìˆ˜ì •] ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ë‚´ì‹¬ ìƒí–¥
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=20, verbose=True  # ë” ì§„ë“í•˜ê²Œ ê¸°ë‹¤ë¦¼
    )
    criterion = nn.MSELoss()

    # Mixed Precision Scaler (GPU ì‚¬ìš© ì‹œë§Œ)
    scaler = torch.amp.GradScaler('cuda') if use_cuda else None
    
    # ========== ìƒˆë¡œìš´ CSV ë° ê·¸ë˜í”„ íŒŒì¼ ìƒì„± ==========
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f'transfer_stage2_metrics_{timestamp}.csv'
    graph_filename = f'transfer_stage2_metrics_{timestamp}.png'
    history = []
    start_epoch = 0
    
    print(f"\nğŸ“ ì „ì´ í•™ìŠµ 2ë‹¨ê³„ ì‹œì‘")
    print(f"  ğŸ’¾ CSV íŒŒì¼: {csv_filename}")
    print(f"  ğŸ“Š ê·¸ë˜í”„ íŒŒì¼: {graph_filename}")
    
    # CSV í—¤ë” ì‘ì„±
    with open(csv_filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Epoch', 'Train_MSE_Loss', 'Train_Box_Loss', 'Train_Mean_IoU', 'Train_Box_Hit_Rate', 'Train_MPE', 'Train_X_MAE', 'Train_Y_MAE', 'Train_PCK@5px', 'Train_PCK@10px', 'Train_Max_Error',
                        'Val_MSE_Loss', 'Val_Box_Loss', 'Val_Mean_IoU', 'Val_Box_Hit_Rate', 'Val_MPE', 'Val_X_MAE', 'Val_Y_MAE', 'Val_PCK@5px', 'Val_PCK@10px', 'Val_Max_Error'])
    
    # ========== í•™ìŠµ ì„¤ì • ==========
    best_val_loss = float('inf')
    best_epoch = 0
    patience_limit = 70  # Early stopping patience
    patience_counter = 0
    best_model_path = 'cup_model_best_stage2.pth'
    latest_model_path = 'cup_model_latest_stage2.pth'
    epochs = 300  # 300 ì—í¬í¬ ëª©í‘œ

    # ========== í•™ìŠµ ë£¨í”„ (Early Stopping í¬í•¨) ==========
    print("\n" + "=" * 100)
    print("ğŸ”¥ ì „ì´ í•™ìŠµ 2ë‹¨ê³„ ì‹œì‘! (Batch Size: 16, Epochs: 300, LR: 0.00005)")
    print(f"  ğŸ“Š Train Batches: {len(train_loader)} / Val Batches: {len(val_loader)}")
    print(f"  ğŸ“ Epoch: 1 ~ {epochs}")
    print(f"  ğŸ’¾ CSV: {csv_filename}")
    print(f"  ğŸ“Š ê·¸ë˜í”„: {graph_filename}")
    print(f"  âš ï¸  Early Stopping: Patience {patience_limit} epochs (Val Loss ê¸°ì¤€)")
    print(f"  ğŸ¯ Loss ê°€ì¤‘ì¹˜: Box=5.0x, Point=5.0x")
    print("=" * 100 + "\n")

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        for images, boxes, points in train_loader:
            images = images.to(device, non_blocking=use_cuda)
            boxes = boxes.to(device, non_blocking=use_cuda)
            points = points.to(device, non_blocking=use_cuda)
            
            # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
            optimizer.zero_grad()
            
            # Mixed Precisionìœ¼ë¡œ í•™ìŠµ (GPU ì‚¬ìš©ì‹œë§Œ)
            if use_cuda:
                with torch.amp.autocast('cuda'):
                    p_boxes, p_points = model(images)
                    box_loss = criterion(p_boxes, boxes)
                    point_loss = criterion(p_points, points)
                    # [ìˆ˜ì •] í•™ìŠµ ë£¨í”„ ë‚´ Loss ê³„ì‚° ë¶€ë¶„
                    # ë°•ìŠ¤ëŠ” í˜•íƒœë¥¼ ê³ ì •(5.0), ì¤‘ì‹¬ì ì€ í”½ì…€ ì •ë°€ë„ íƒ€ê²©(5.0)
                    loss = (5.0 * box_loss) + (5.0 * point_loss)
                
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                p_boxes, p_points = model(images)
                box_loss = criterion(p_boxes, boxes)
                point_loss = criterion(p_points, points)
                # [ìˆ˜ì •] í•™ìŠµ ë£¨í”„ ë‚´ Loss ê³„ì‚° ë¶€ë¶„
                # ë°•ìŠ¤ëŠ” í˜•íƒœë¥¼ ê³ ì •(5.0), ì¤‘ì‹¬ì ì€ í”½ì…€ ì •ë°€ë„ íƒ€ê²©(5.0)
                loss = (5.0 * box_loss) + (5.0 * point_loss)
                loss.backward()
                optimizer.step()
            
            total_loss += loss.item()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if use_cuda:
            torch.cuda.empty_cache()

        # ì—í¬í¬ ì¢…ë£Œ í›„ í‰ê°€
        train_metrics = calculate_metrics(model, train_loader, device, img_size=640)
        val_metrics = calculate_metrics(model, val_loader, device, img_size=640)
        
        # Learning Rate Scheduler ì—…ë°ì´íŠ¸
        scheduler.step(val_metrics['mse_loss'])
        
        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        combined_metrics = {'epoch': epoch + 1}
        combined_metrics.update({f'train_{k}': v for k, v in train_metrics.items()})
        combined_metrics.update({f'val_{k}': v for k, v in val_metrics.items()})
        history.append(combined_metrics)
        
        # CMD ì¶œë ¥ (ë§¤ ì—í¬í¬ë§ˆë‹¤ ì¶œë ¥)
        print(f"\n[Epoch {epoch+1}/{epochs}]")
        print(f"  [Train] Total Loss: {train_metrics['mse_loss']:.6f} | Box Loss: {train_metrics['box_loss']:.6f} | MPE: {train_metrics['mpe']:.2f}px")
        print(f"  [Train] Mean IoU: {train_metrics['mean_iou']:.4f} | Box Hit Rate: {train_metrics['box_hit_rate']:.2f}% | PCK@5px: {train_metrics['pck_5']:.2f}%")
        print(f"  [Val]   Total Loss: {val_metrics['mse_loss']:.6f} | Box Loss: {val_metrics['box_loss']:.6f} | MPE: {val_metrics['mpe']:.2f}px")
        print(f"  [Val]   Mean IoU: {val_metrics['mean_iou']:.4f} | Box Hit Rate: {val_metrics['box_hit_rate']:.2f}% | PCK@5px: {val_metrics['pck_5']:.2f}% â­")
        print(f"  [LR]    {optimizer.param_groups[0]['lr']:.8f}")
        print("-" * 100)
        
        # CSVì— ì €ì¥
        with open(csv_filename, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                combined_metrics['epoch'],
                f"{train_metrics['mse_loss']:.6f}",
                f"{train_metrics['box_loss']:.6f}",
                f"{train_metrics['mean_iou']:.4f}",
                f"{train_metrics['box_hit_rate']:.2f}",
                f"{train_metrics['mpe']:.2f}",
                f"{train_metrics['x_mae']:.2f}",
                f"{train_metrics['y_mae']:.2f}",
                f"{train_metrics['pck_5']:.2f}",
                f"{train_metrics['pck_10']:.2f}",
                f"{train_metrics['max_error']:.2f}",
                f"{val_metrics['mse_loss']:.6f}",
                f"{val_metrics['box_loss']:.6f}",
                f"{val_metrics['mean_iou']:.4f}",
                f"{val_metrics['box_hit_rate']:.2f}",
                f"{val_metrics['mpe']:.2f}",
                f"{val_metrics['x_mae']:.2f}",
                f"{val_metrics['y_mae']:.2f}",
                f"{val_metrics['pck_5']:.2f}",
                f"{val_metrics['pck_10']:.2f}",
                f"{val_metrics['max_error']:.2f}"
            ])
        
        # ê·¸ë˜í”„ ìƒì„± (ë§¤ ì—í¬í¬ë§ˆë‹¤ ë®ì–´ì“°ê¸°)
        val_history = [{'epoch': h['epoch'], **{k.replace('val_', ''): v for k, v in h.items() if k.startswith('val_')}} for h in history]
        plot_training_metrics(val_history, save_path=graph_filename, best_epoch=best_epoch)
        
        # ëª¨ë¸ ì €ì¥
        # 1. ìµœì‹  ëª¨ë¸ ì €ì¥ (ë®ì–´ì“°ê¸°)
        torch.save(model.state_dict(), latest_model_path)
        
        # 2. Best ëª¨ë¸ ì €ì¥ + Early Stopping
        if val_metrics['mse_loss'] < best_val_loss:
            best_val_loss = val_metrics['mse_loss']
            best_epoch = epoch + 1
            patience_counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹
            torch.save(model.state_dict(), best_model_path)
            print(f"  â­ Best ëª¨ë¸ ì €ì¥! (Val Loss: {best_val_loss:.6f})")
        else:
            patience_counter += 1
            print(f"  âš ï¸  Early Stopping Counter: {patience_counter}/{patience_limit}")
            
            # Early Stopping ë°œë™
            if patience_counter >= patience_limit:
                print(f"\nğŸ›‘ Early Stopping ë°œë™! (Patience {patience_limit} epochs ë„ë‹¬)")
                print(f"  Best Epoch: {best_epoch} (Val Loss: {best_val_loss:.6f})")
                print(f"  í˜„ì¬ Epoch: {epoch + 1}")
                break
    
    # ========== ìµœì¢… ë³´ê³ ì„œ ==========
    print("\n" + "=" * 100)
    print("ğŸ‰ ì „ì´ í•™ìŠµ 2ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("=" * 100)
    
    # ìµœì¢… ë©”íŠ¸ë¦­ ì¶œë ¥
    final_metrics = history[-1]
    print(f"\n[ìµœì¢… ì„±ëŠ¥ (Epoch {final_metrics['epoch']}) - Validation Set]")
    print(f"\nğŸ“¦ ë°•ìŠ¤ ê²€ì¶œ ì„±ëŠ¥:")
    print(f"  Box Loss (MSE)     : {final_metrics['val_box_loss']:.6f}")
    print(f"  Mean IoU (mIoU)    : {final_metrics['val_mean_iou']:.4f} {'âœ… ëª©í‘œë‹¬ì„±!' if final_metrics['val_mean_iou'] >= 0.7 else ''}")
    print(f"  Box Hit Rate       : {final_metrics['val_box_hit_rate']:.2f}%")
    print(f"\nğŸ¯ ì¤‘ì‹¬ì  ê²€ì¶œ ì„±ëŠ¥:")
    print(f"  Total Loss (ê°€ì¤‘)  : {final_metrics['val_mse_loss']:.6f}")
    print(f"  MPE                : {final_metrics['val_mpe']:.2f} px")
    print(f"  X MAE              : {final_metrics['val_x_mae']:.2f} px")
    print(f"  Y MAE              : {final_metrics['val_y_mae']:.2f} px")
    print(f"  PCK @ 5px          : {final_metrics['val_pck_5']:.2f}%")
    print(f"  PCK @ 10px         : {final_metrics['val_pck_10']:.2f}%")
    print(f"  Max Error          : {final_metrics['val_max_error']:.2f} px")
    
    # ìµœì¢… ê·¸ë˜í”„ ìƒì„±
    val_history = [{'epoch': h['epoch'], **{k.replace('val_', ''): v for k, v in h.items() if k.startswith('val_')}} for h in history]
    plot_training_metrics(val_history, save_path=graph_filename, best_epoch=best_epoch)
    
    print(f"\nğŸ’¾ ì €ì¥ëœ íŒŒì¼:")
    print(f"  - ìµœì‹  ëª¨ë¸: {latest_model_path} (ë§ˆì§€ë§‰ ì—í¬í¬)")
    print(f"  - ìµœê³  ëª¨ë¸: {best_model_path} (Epoch {best_epoch}, Val Loss: {best_val_loss:.6f})")
    print(f"  - ë©”íŠ¸ë¦­ CSV: {csv_filename}")
    print(f"  - í•™ìŠµ ê·¸ë˜í”„: {graph_filename}")
    print(f"\nğŸ’¡ ì‚¬ìš© íŒ:")
    print(f"  - Best ëª¨ë¸({best_model_path})ì„ í”„ë¡œë•ì…˜ì— ì‚¬ìš©í•˜ì„¸ìš”")
    print(f"  - Early Stoppingìœ¼ë¡œ ê³¼ì í•© ë°©ì§€ (Best: Epoch {best_epoch})")
    print(f"  - ì´ì „ cup_model_best.pthì—ì„œ ì‹œì‘í•œ ì •ë°€ íŠœë‹ ê²°ê³¼ì…ë‹ˆë‹¤")
    print("=" * 100)

if __name__ == "__main__":
    main()
